{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnull, when, count, countDistinct\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.types import IntegerType, FloatType, DoubleType\n",
    "import mlflow\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/27 09:44:50 WARN Utils: Your hostname, Ghazis-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 166.87.130.137 instead (on interface en0)\n",
      "24/11/27 09:44:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/27 09:44:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Configure SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark_MLlib_GBT_Label_Separation\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MLflow for local tracking\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")  # Use local directory for tracking\n",
    "experiment_name = \"MLlib_GBTClassifier\"\n",
    "\n",
    "# Create the experiment if it doesn't exist\n",
    "if not mlflow.get_experiment_by_name(experiment_name):\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Configure paths for saving models and logs\n",
    "MODEL_SAVE_DIR = \"./models\"\n",
    "LOG_DIR = \"./logs\"\n",
    "\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Reference Table\n",
    "REFERENCE_TABLE_PATH = \"../EDA/Results/reference_table_real.csv\"\n",
    "reference_table_missing_values_real = spark.read.csv(REFERENCE_TABLE_PATH, header=True)\n",
    "\n",
    "# Define feature names excluding \"DataType\"\n",
    "feature_names = [\n",
    "    row[\"Tag\"] for row in reference_table_missing_values_real.filter(col(\"Value Type\").isin([\"Continuous\", \"Categorical\"])).collect()\n",
    "    if row[\"Tag\"] != \"DataType\"\n",
    "]\n",
    "target_name = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datasets into PySpark DataFrames\n",
    "data_dir = \"../Cleaning & Preparation/Train Test (Scaled) Data\"\n",
    "train_data_spark = spark.read.parquet(os.path.join(data_dir, \"scaled_train_data.parquet\"))\n",
    "test_data_spark = spark.read.parquet(os.path.join(data_dir, \"scaled_test_data.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of labels to process\n",
    "labels_to_process = [1, 2, 3, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data by label into PySpark DataFrames\n",
    "label_dataframes = {}\n",
    "for label in labels_to_process:\n",
    "    train_df_label = train_data_spark.filter(col(\"label\") == label)\n",
    "    test_df_label = test_data_spark.filter(col(\"label\") == label)\n",
    "\n",
    "    label_dataframes[label] = {\n",
    "        \"train\": train_df_label,\n",
    "        \"test\": test_df_label,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/27 09:44:59 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 1_train: No nulls, all columns have valid types.\n",
      "Label 1_test: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 2_train: No nulls, all columns have valid types.\n",
      "Label 2_test: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 3_train: No nulls, all columns have valid types.\n",
      "Label 3_test: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 4_train: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 4_test: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 5_train: No nulls, all columns have valid types.\n",
      "Label 5_test: No nulls, all columns have valid types.\n",
      "Label 6_train: No nulls, all columns have valid types.\n",
      "Label 6_test: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 7_train: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 7_test: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 8_train: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 8_test: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 9_train: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:====================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 9_test: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Function to verify data integrity\n",
    "def verify_data(df, label):\n",
    "    # Check for null values\n",
    "    null_counts = df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "    if any(null_counts.values()):\n",
    "        print(f\"Null values detected in label {label}: {null_counts}\")\n",
    "        raise ValueError(f\"Null values detected in label {label}. Investigate the data.\")\n",
    "\n",
    "    # Verify column types\n",
    "    for col_name in feature_names:\n",
    "        col_type = [f.dataType for f in df.schema.fields if f.name == col_name][0]\n",
    "        if not isinstance(col_type, (IntegerType, FloatType, DoubleType)):\n",
    "            print(f\"Column {col_name} in label {label} is of type {col_type}, expected numeric.\")\n",
    "            raise ValueError(f\"Column {col_name} in label {label} is not numeric.\")\n",
    "\n",
    "    print(f\"Label {label}: No nulls, all columns have valid types.\")\n",
    "\n",
    "# Separate the execution of the verification from the data separation\n",
    "for label in labels_to_process:\n",
    "    train_df_label = label_dataframes[label]['train']\n",
    "    test_df_label = label_dataframes[label]['test']\n",
    "\n",
    "    verify_data(train_df_label, f\"{label}_train\")\n",
    "    verify_data(test_df_label, f\"{label}_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/27 09:47:14 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC on test set for label 1: 0.5\n",
      "AUC-PR on test set for label 1: 0.17106088046809698\n",
      "Test Precision for label 1: 0\n",
      "Test Recall for label 1: 0.0\n",
      "Test F1 Score for label 1: 0\n",
      "Test Confusion Matrix: {'tp': 0, 'tn': 47601, 'fp': 0, 'fn': 9823}\n",
      "\n",
      "Processing label: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC on test set for label 2: 0.9682319612905145\n",
      "AUC-PR on test set for label 2: 0.9325770747361606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision for label 2: 0.8668590593557445\n",
      "Test Recall for label 2: 0.9535003431708992\n",
      "Test F1 Score for label 2: 0.908117824896842\n",
      "Test Confusion Matrix: {'tp': 22228, 'tn': 53801, 'fp': 3414, 'fn': 1084}\n",
      "\n",
      "Processing label: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC on test set for label 3: 0.42859075657552387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-PR on test set for label 3: 0.8008437719712442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision for label 3: 0.8200074996875131\n",
      "Test Recall for label 3: 0.7839848947171345\n",
      "Test F1 Score for label 3: 0.8015916977566349\n",
      "Test Confusion Matrix: {'tp': 98405, 'tn': 3600, 'fp': 21600, 'fn': 27114}\n",
      "\n",
      "Processing label: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC on test set for label 4: 0.373401400650308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-PR on test set for label 4: 0.5814253772251248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision for label 4: 0.6605895035983383\n",
      "Test Recall for label 4: 0.9742251640060232\n",
      "Test F1 Score for label 4: 0.7873221720316692\n",
      "Test Confusion Matrix: {'tp': 480709, 'tn': 1412, 'fp': 246988, 'fn': 12718}\n",
      "\n",
      "Processing label: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC on test set for label 5: 0.806431312061525\n",
      "AUC-PR on test set for label 5: 0.8573871110930698\n",
      "Test Precision for label 5: 0.9032047622930768\n",
      "Test Recall for label 5: 0.5128965038203288\n",
      "Test F1 Score for label 5: 0.6542619174198122\n",
      "Test Confusion Matrix: {'tp': 22152, 'tn': 13183, 'fp': 2374, 'fn': 21038}\n",
      "\n",
      "Processing label: 6\n",
      "AUC-ROC on test set for label 6: 0.3375404226943844\n",
      "AUC-PR on test set for label 6: 0.05199628955055645\n",
      "Test Precision for label 6: 0.04787487249234954\n",
      "Test Recall for label 6: 0.4785859959211421\n",
      "Test F1 Score for label 6: 0.08704253214638973\n",
      "Test Confusion Matrix: {'tp': 704, 'tn': 5624, 'fp': 14001, 'fn': 767}\n",
      "\n",
      "Processing label: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC on test set for label 7: 0.6916148994233025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-PR on test set for label 7: 0.9019469325467595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision for label 7: 0.8741424027847383\n",
      "Test Recall for label 7: 0.9788198859082037\n",
      "Test F1 Score for label 7: 0.9235244259232395\n",
      "Test Confusion Matrix: {'tp': 1254112, 'tn': 19806, 'fp': 180565, 'fn': 27137}\n",
      "\n",
      "Processing label: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC on test set for label 8: 0.9100174262969691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-PR on test set for label 8: 0.9268066091163721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision for label 8: 0.9147258991008991\n",
      "Test Recall for label 8: 0.9993898406574562\n",
      "Test F1 Score for label 8: 0.9551854692580958\n",
      "Test Confusion Matrix: {'tp': 527409, 'tn': 92506, 'fp': 49167, 'fn': 322}\n",
      "\n",
      "Processing label: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC on test set for label 9: 0.9046984996914098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-PR on test set for label 9: 0.5921780657880655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision for label 9: 0.4309350272133773\n",
      "Test Recall for label 9: 0.34970811763410065\n",
      "Test F1 Score for label 9: 0.3860956909288793\n",
      "Test Confusion Matrix: {'tp': 31750, 'tn': 589990, 'fp': 41927, 'fn': 59040}\n",
      "\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Train a GBT model for each label\n",
    "for label in labels_to_process:\n",
    "    print(f\"\\nProcessing label: {label}\")\n",
    "\n",
    "    train_df = label_dataframes[label][\"train\"]\n",
    "    test_df = label_dataframes[label][\"test\"]\n",
    "\n",
    "    # Check if there's sufficient data\n",
    "    if train_df.count() == 0 or test_df.count() == 0:\n",
    "        print(f\"Insufficient data for label {label}. Skipping this label.\")\n",
    "        continue\n",
    "\n",
    "    # Assemble features into a feature vector\n",
    "    assembler = VectorAssembler(inputCols=feature_names, outputCol=\"features\")\n",
    "    train_df = assembler.transform(train_df)\n",
    "    test_df = assembler.transform(test_df)\n",
    "\n",
    "    # Adjust model training section to include hyper-parameters with default values\n",
    "    maxDepth = 5            # Maximum depth of the tree (default=5). Controls overfitting.\n",
    "    maxBins = 32            # Maximum number of bins used for splitting features (default=32).\n",
    "    maxIter = 20            # Number of iterations (trees) (default=20).\n",
    "    stepSize = 0.1          # Learning rate (default=0.1). Controls the rate at which the model learns.\n",
    "    subsamplingRate = 1.0   # Fraction of data to use for training each tree (default=1.0).\n",
    "    lossType = \"logistic\"   # Loss function to minimize (default='logistic').\n",
    "    minInstancesPerNode = 1 # Minimum instances per node (default=1).\n",
    "    minInfoGain = 0.0       # Minimum information gain for a split (default=0.0).\n",
    "    seed = 123              # Random seed for reproducibility (default=None).\n",
    "\n",
    "    # Define the GBTClassifier with hyperparameters\n",
    "    gbt = GBTClassifier(featuresCol=\"features\", labelCol=target_name,\n",
    "                        maxDepth=maxDepth,\n",
    "                        maxBins=maxBins,\n",
    "                        maxIter=maxIter,\n",
    "                        stepSize=stepSize,\n",
    "                        subsamplingRate=subsamplingRate,\n",
    "                        lossType=lossType,\n",
    "                        minInstancesPerNode=minInstancesPerNode,\n",
    "                        minInfoGain=minInfoGain,\n",
    "                        seed=seed)\n",
    "\n",
    "    # Start an MLflow run\n",
    "    with mlflow.start_run(run_name=f\"GBT_Label_{label}\"):\n",
    "        # Train the model on the training set\n",
    "        gbt_model = gbt.fit(train_df)\n",
    "\n",
    "        # Predict on the test set\n",
    "        test_predictions = gbt_model.transform(test_df)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        evaluator_roc = BinaryClassificationEvaluator(labelCol=target_name, metricName=\"areaUnderROC\")\n",
    "        auc_roc_test = evaluator_roc.evaluate(test_predictions)\n",
    "        print(f\"AUC-ROC on test set for label {label}: {auc_roc_test}\")\n",
    "\n",
    "        evaluator_pr = BinaryClassificationEvaluator(labelCol=target_name, metricName=\"areaUnderPR\")\n",
    "        auc_pr_test = evaluator_pr.evaluate(test_predictions)\n",
    "        print(f\"AUC-PR on test set for label {label}: {auc_pr_test}\")\n",
    "\n",
    "        # Include classification report and confusion matrix as evaluation results\n",
    "        def compute_confusion_matrix(predictions, label_col, prediction_col):\n",
    "            tp = predictions.filter((col(label_col) == 1) & (col(prediction_col) == 1)).count()\n",
    "            tn = predictions.filter((col(label_col) == 0) & (col(prediction_col) == 0)).count()\n",
    "            fp = predictions.filter((col(label_col) == 0) & (col(prediction_col) == 1)).count()\n",
    "            fn = predictions.filter((col(label_col) == 1) & (col(prediction_col) == 0)).count()\n",
    "            return {'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn}\n",
    "\n",
    "        confusion_matrix_test = compute_confusion_matrix(test_predictions, target_name, 'prediction')\n",
    "\n",
    "        # Compute key metrics for test set\n",
    "        tp_test = confusion_matrix_test['tp']\n",
    "        tn_test = confusion_matrix_test['tn']\n",
    "        fp_test = confusion_matrix_test['fp']\n",
    "        fn_test = confusion_matrix_test['fn']\n",
    "        precision_test = tp_test / (tp_test + fp_test) if (tp_test + fp_test) > 0 else 0\n",
    "        recall_test = tp_test / (tp_test + fn_test) if (tp_test + fn_test) > 0 else 0\n",
    "        f1_score_test = 2 * precision_test * recall_test / (precision_test + recall_test) if (precision_test + recall_test) > 0 else 0\n",
    "\n",
    "        print(f\"Test Precision for label {label}: {precision_test}\")\n",
    "        print(f\"Test Recall for label {label}: {recall_test}\")\n",
    "        print(f\"Test F1 Score for label {label}: {f1_score_test}\")\n",
    "        print(f\"Test Confusion Matrix: {confusion_matrix_test}\")\n",
    "\n",
    "        # Capture the feature importance for each model\n",
    "        importances = gbt_model.featureImportances\n",
    "        feature_importance_list = []\n",
    "        for idx, imp in enumerate(importances):\n",
    "            feature_importance_list.append((feature_names[idx], imp))\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        feature_importance_df = pd.DataFrame(feature_importance_list, columns=[\"feature\", \"importance\"])\n",
    "\n",
    "        # Save feature importances to a file\n",
    "        feature_importance_path = os.path.join(LOG_DIR, f\"feature_importance_label_{label}.csv\")\n",
    "        feature_importance_df.to_csv(feature_importance_path, index=False)\n",
    "        mlflow.log_artifact(feature_importance_path, artifact_path=\"feature_importance\")\n",
    "\n",
    "        # Save the model\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f\"gbt_model_label_{label}\")\n",
    "        gbt_model.write().overwrite().save(model_path)\n",
    "        mlflow.log_artifact(model_path, artifact_path=\"models\")\n",
    "\n",
    "        # Log parameters and metrics to MLflow\n",
    "        mlflow.log_param(\"label\", label)\n",
    "        mlflow.log_param(\"maxDepth\", maxDepth)\n",
    "        mlflow.log_param(\"maxBins\", maxBins)\n",
    "        mlflow.log_param(\"maxIter\", maxIter)\n",
    "        mlflow.log_param(\"stepSize\", stepSize)\n",
    "        mlflow.log_param(\"subsamplingRate\", subsamplingRate)\n",
    "        mlflow.log_param(\"lossType\", lossType)\n",
    "        mlflow.log_param(\"minInstancesPerNode\", minInstancesPerNode)\n",
    "        mlflow.log_param(\"minInfoGain\", minInfoGain)\n",
    "        mlflow.log_param(\"seed\", seed)\n",
    "\n",
    "        # Log test metrics\n",
    "        mlflow.log_metric(\"auc_roc_test\", auc_roc_test)\n",
    "        mlflow.log_metric(\"auc_pr_test\", auc_pr_test)\n",
    "        mlflow.log_metric(\"precision_test\", precision_test)\n",
    "        mlflow.log_metric(\"recall_test\", recall_test)\n",
    "        mlflow.log_metric(\"f1_score_test\", f1_score_test)\n",
    "        mlflow.log_metric(\"true_positives_test\", tp_test)\n",
    "        mlflow.log_metric(\"true_negatives_test\", tn_test)\n",
    "        mlflow.log_metric(\"false_positives_test\", fp_test)\n",
    "        mlflow.log_metric(\"false_negatives_test\", fn_test)\n",
    "\n",
    "        # End the MLflow run\n",
    "        mlflow.end_run()\n",
    "\n",
    "print(\"\\nProcessing complete.\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
