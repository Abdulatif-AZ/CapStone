{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnull, when, count\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.types import IntegerType, FloatType, DoubleType\n",
    "import mlflow\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/26 19:22:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Configure SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark_MLlib_GBT_Label_Separation\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MLflow for local tracking\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")  # Use local directory for tracking\n",
    "experiment_name = \"MLlib_GBTClassifier\"\n",
    "\n",
    "# Create the experiment if it doesn't exist\n",
    "if not mlflow.get_experiment_by_name(experiment_name):\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Configure paths for saving models and logs\n",
    "MODEL_SAVE_DIR = \"./models\"\n",
    "LOG_DIR = \"./logs\"\n",
    "\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Reference Table\n",
    "REFERENCE_TABLE_PATH = \"../EDA/Results/reference_table_real.csv\"\n",
    "reference_table_missing_values_real = spark.read.csv(REFERENCE_TABLE_PATH, header=True)\n",
    "\n",
    "# Define feature names excluding \"DataType\"\n",
    "feature_names = [\n",
    "    row[\"Tag\"] for row in reference_table_missing_values_real.filter(col(\"Value Type\").isin([\"Continuous\", \"Categorical\"])).collect()\n",
    "    if row[\"Tag\"] != \"DataType\"\n",
    "]\n",
    "target_name = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datasets into PySpark DataFrames\n",
    "data_dir = \"../Cleaning & Preparation/Train Test (Scaled) Data\"\n",
    "train_data_spark = spark.read.parquet(os.path.join(data_dir, \"scaled_train_data.parquet\"))\n",
    "test_data_spark = spark.read.parquet(os.path.join(data_dir, \"scaled_test_data.parquet\"))\n",
    "validation_data_spark = spark.read.parquet(os.path.join(data_dir, \"scaled_validation_data.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing -1 with 0 in column target...\n",
      "Replacing -1 with 0 in column target...\n",
      "Replacing -1 with 0 in column target...\n"
     ]
    }
   ],
   "source": [
    "# Function to replace -1 labels with 0\n",
    "def replace_invalid_labels(df, label_name):\n",
    "    print(f\"Replacing -1 with 0 in column {label_name}...\")\n",
    "    df = df.withColumn(label_name, when(col(label_name) == -1, 0).otherwise(col(label_name)))\n",
    "    return df\n",
    "\n",
    "# Replace all -1 with 0 in 'target' column right after loading\n",
    "train_data_spark = replace_invalid_labels(train_data_spark, target_name)\n",
    "test_data_spark = replace_invalid_labels(test_data_spark, target_name)\n",
    "validation_data_spark = replace_invalid_labels(validation_data_spark, target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of labels to process\n",
    "labels_to_process = [1, 2, 3, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data by label into PySpark DataFrames\n",
    "label_dataframes = {}\n",
    "for label in labels_to_process:\n",
    "    train_df_label = train_data_spark.filter(col(\"label\") == label)\n",
    "    validation_df_label = validation_data_spark.filter(col(\"label\") == label)\n",
    "    test_df_label = test_data_spark.filter(col(\"label\") == label)\n",
    "\n",
    "    label_dataframes[label] = {\n",
    "        \"train\": train_df_label,\n",
    "        \"validation\": validation_df_label,\n",
    "        \"test\": test_df_label,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/26 19:23:02 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 1_train: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 1_validation: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 1_test: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 2_train: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 2_validation: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 2_test: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 3_train: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 3_validation: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 3_test: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 4_train: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 4_validation: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 4_test: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 5_train: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 5_validation: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 5_test: No nulls, all columns have valid types.\n",
      "Label 6_train: No nulls, all columns have valid types.\n",
      "Label 6_validation: No nulls, all columns have valid types.\n",
      "Label 6_test: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 7_train: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 7_validation: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 7_test: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 8_train: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 8_validation: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 8_test: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 9_train: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 9_validation: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 83:====================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 9_test: No nulls, all columns have valid types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Function to verify data integrity\n",
    "def verify_data(df, label):\n",
    "    # Check for null values\n",
    "    null_counts = df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "    if any(null_counts.values()):\n",
    "        print(f\"Null values detected in label {label}: {null_counts}\")\n",
    "        raise ValueError(f\"Null values detected in label {label}. Investigate the data.\")\n",
    "\n",
    "    # Verify column types\n",
    "    for col_name in feature_names:\n",
    "        col_type = [f.dataType for f in df.schema.fields if f.name == col_name][0]\n",
    "        if not isinstance(col_type, (IntegerType, FloatType, DoubleType)):\n",
    "            print(f\"Column {col_name} in label {label} is of type {col_type}, expected numeric.\")\n",
    "            raise ValueError(f\"Column {col_name} in label {label} is not numeric.\")\n",
    "\n",
    "    print(f\"Label {label}: No nulls, all columns have valid types.\")\n",
    "\n",
    "# Separate the execution of the verification from the data separation\n",
    "for label in labels_to_process:\n",
    "    train_df_label = label_dataframes[label]['train']\n",
    "    validation_df_label = label_dataframes[label]['validation']\n",
    "    test_df_label = label_dataframes[label]['test']\n",
    "\n",
    "    verify_data(train_df_label, f\"{label}_train\")\n",
    "    verify_data(validation_df_label, f\"{label}_validation\")\n",
    "    verify_data(test_df_label, f\"{label}_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/26 19:26:20 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for label 1: 0.9984564236676148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for label 1: 0.9973010093266896\n",
      "Recall for label 1: 0.9557239057239058\n",
      "F1 Score for label 1: 0.9760698990277908\n",
      "Confusion Matrix: {'tp': 62447, 'tn': 171285, 'fp': 169, 'fn': 2893}\n",
      "\n",
      "Processing label: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for label 2: 0.9926040254803992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for label 2: 0.9720844374985844\n",
      "Recall for label 2: 0.9634755474739312\n",
      "F1 Score for label 2: 0.9677608473840569\n",
      "Confusion Matrix: {'tp': 85837, 'tn': 185445, 'fp': 2465, 'fn': 3254}\n",
      "\n",
      "Processing label: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for label 3: 0.9648958338147932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for label 3: 0.8532301544225654\n",
      "Recall for label 3: 0.9971027071854267\n",
      "F1 Score for label 3: 0.9195730287010593\n",
      "Confusion Matrix: {'tp': 567503, 'tn': 17580, 'fp': 97620, 'fn': 1649}\n",
      "\n",
      "Processing label: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for label 4: 0.7341298606762959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for label 4: 0.6681818045540264\n",
      "Recall for label 4: 0.998638631657802\n",
      "F1 Score for label 4: 0.8006527259948634\n",
      "Confusion Matrix: {'tp': 2451541, 'tn': 17368, 'fp': 1217432, 'fn': 3342}\n",
      "\n",
      "Processing label: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for label 5: 0.9983420039940153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for label 5: 0.9670473482242505\n",
      "Recall for label 5: 0.9862663406845624\n",
      "F1 Score for label 5: 0.9765622949528885\n",
      "Confusion Matrix: {'tp': 316268, 'tn': 107959, 'fp': 10777, 'fn': 4404}\n",
      "\n",
      "Processing label: 6\n",
      "AUC for label 6: 0.9121168816076213\n",
      "Precision for label 6: 0.9818982387475538\n",
      "Recall for label 6: 0.6110519104886588\n",
      "F1 Score for label 6: 0.7533076850896124\n",
      "Confusion Matrix: {'tp': 4014, 'tn': 70834, 'fp': 74, 'fn': 2555}\n",
      "\n",
      "Processing label: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for label 7: 0.9919171481531437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for label 7: 0.9271423816197853\n",
      "Recall for label 7: 0.9816704149652908\n",
      "F1 Score for label 7: 0.9536275617230621\n",
      "Confusion Matrix: {'tp': 6548313, 'tn': 679777, 'fp': 514586, 'fn': 122269}\n",
      "\n",
      "Processing label: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for label 8: 0.9977719983934743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for label 8: 0.9733505042756959\n",
      "Recall for label 8: 0.9740694497865907\n",
      "F1 Score for label 8: 0.9737098443215431\n",
      "Confusion Matrix: {'tp': 3560631, 'tn': 1056130, 'fp': 97487, 'fn': 94787}\n",
      "\n",
      "Processing label: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for label 9: 0.9763970731502984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for label 9: 0.9158861331305223\n",
      "Recall for label 9: 0.918088649887691\n",
      "F1 Score for label 9: 0.9169860689500097\n",
      "Confusion Matrix: {'tp': 179434, 'tn': 2423450, 'fp': 16479, 'fn': 16009}\n",
      "\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Train a GBT model for each label\n",
    "for label in labels_to_process:\n",
    "    print(f\"\\nProcessing label: {label}\")\n",
    "\n",
    "    train_df = label_dataframes[label][\"train\"]\n",
    "    validation_df = label_dataframes[label][\"validation\"]\n",
    "    test_df = label_dataframes[label][\"test\"]\n",
    "\n",
    "    # Check if there's sufficient data\n",
    "    if train_df.count() == 0 or validation_df.count() == 0 or test_df.count() == 0:\n",
    "        print(f\"Insufficient data for label {label}. Skipping this label.\")\n",
    "        continue\n",
    "\n",
    "    # Assemble features into a feature vector\n",
    "    assembler = VectorAssembler(inputCols=feature_names, outputCol=\"features\")\n",
    "    train_df = assembler.transform(train_df)\n",
    "    validation_df = assembler.transform(validation_df)\n",
    "    test_df = assembler.transform(test_df)\n",
    "\n",
    "    # Adjust model training section to include hyper-parameters with default values\n",
    "    maxDepth = 5  # Maximum depth of the tree (default=5). Controls overfitting.\n",
    "    maxBins = 32  # Maximum number of bins used for splitting features (default=32).\n",
    "    maxIter = 20  # Number of iterations (trees) (default=20).\n",
    "    stepSize = 0.1  # Learning rate (default=0.1). Controls the rate at which the model learns.\n",
    "    subsamplingRate = 1.0  # Fraction of data to use for training each tree (default=1.0).\n",
    "    lossType = \"logistic\"  # Loss function to minimize (default='logistic').\n",
    "    minInstancesPerNode = 1  # Minimum instances per node (default=1).\n",
    "    minInfoGain = 0.0  # Minimum information gain for a split (default=0.0).\n",
    "\n",
    "    # Define the GBTClassifier with hyperparameters\n",
    "    gbt = GBTClassifier(featuresCol=\"features\", labelCol=target_name,\n",
    "                        maxDepth=maxDepth,\n",
    "                        maxBins=maxBins,\n",
    "                        maxIter=maxIter,\n",
    "                        stepSize=stepSize,\n",
    "                        subsamplingRate=subsamplingRate,\n",
    "                        lossType=lossType,\n",
    "                        minInstancesPerNode=minInstancesPerNode,\n",
    "                        minInfoGain=minInfoGain)\n",
    "\n",
    "    # Start an MLflow run\n",
    "    with mlflow.start_run(run_name=f\"GBT_Label_{label}\"):\n",
    "        # Train the model\n",
    "        gbt_model = gbt.fit(train_df)\n",
    "\n",
    "        # Predict on the validation set\n",
    "        validation_predictions = gbt_model.transform(validation_df)\n",
    "\n",
    "        # Evaluate the model\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=target_name, metricName=\"areaUnderPR\")\n",
    "        aupr = evaluator.evaluate(validation_predictions)\n",
    "        print(f\"AUC for label {label}: {aupr}\")\n",
    "\n",
    "        # Include classification report and confusion matrix as evaluation results\n",
    "        def compute_confusion_matrix(predictions, label_col, prediction_col):\n",
    "            tp = predictions.filter((col(label_col) == 1) & (col(prediction_col) == 1)).count()\n",
    "            tn = predictions.filter((col(label_col) == 0) & (col(prediction_col) == 0)).count()\n",
    "            fp = predictions.filter((col(label_col) == 0) & (col(prediction_col) == 1)).count()\n",
    "            fn = predictions.filter((col(label_col) == 1) & (col(prediction_col) == 0)).count()\n",
    "            return {'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn}\n",
    "\n",
    "        confusion_matrix_val = compute_confusion_matrix(validation_predictions, target_name, 'prediction')\n",
    "\n",
    "        # Compute key metrics\n",
    "        tp = confusion_matrix_val['tp']\n",
    "        tn = confusion_matrix_val['tn']\n",
    "        fp = confusion_matrix_val['fp']\n",
    "        fn = confusion_matrix_val['fn']\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        print(f\"Precision for label {label}: {precision}\")\n",
    "        print(f\"Recall for label {label}: {recall}\")\n",
    "        print(f\"F1 Score for label {label}: {f1_score}\")\n",
    "        print(f\"Confusion Matrix: {confusion_matrix_val}\")\n",
    "\n",
    "        # Capture the feature importance for each model\n",
    "        importances = gbt_model.featureImportances\n",
    "        feature_importance_list = []\n",
    "        for idx, imp in enumerate(importances):\n",
    "            feature_importance_list.append((feature_names[idx], imp))\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        feature_importance_df = pd.DataFrame(feature_importance_list, columns=[\"feature\", \"importance\"])\n",
    "\n",
    "        # Save feature importances to a file\n",
    "        feature_importance_path = os.path.join(LOG_DIR, f\"feature_importance_label_{label}.csv\")\n",
    "        feature_importance_df.to_csv(feature_importance_path, index=False)\n",
    "        mlflow.log_artifact(feature_importance_path, artifact_path=\"feature_importance\")\n",
    "\n",
    "        # Save the model\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f\"gbt_model_label_{label}\")\n",
    "        gbt_model.write().overwrite().save(model_path)\n",
    "        mlflow.log_artifact(model_path, artifact_path=\"models\")\n",
    "\n",
    "        # Log parameters and metrics to MLflow\n",
    "        mlflow.log_param(\"label\", label)\n",
    "        mlflow.log_param(\"maxDepth\", maxDepth)\n",
    "        mlflow.log_param(\"maxBins\", maxBins)\n",
    "        mlflow.log_param(\"maxIter\", maxIter)\n",
    "        mlflow.log_param(\"stepSize\", stepSize)\n",
    "        mlflow.log_param(\"subsamplingRate\", subsamplingRate)\n",
    "        mlflow.log_param(\"lossType\", lossType)\n",
    "        mlflow.log_param(\"minInstancesPerNode\", minInstancesPerNode)\n",
    "        mlflow.log_param(\"minInfoGain\", minInfoGain)\n",
    "\n",
    "        mlflow.log_metric(\"areaUnderPR\", aupr)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1_score)\n",
    "        mlflow.log_metric(\"true_positives\", tp)\n",
    "        mlflow.log_metric(\"true_negatives\", tn)\n",
    "        mlflow.log_metric(\"false_positives\", fp)\n",
    "        mlflow.log_metric(\"false_negatives\", fn)\n",
    "\n",
    "        # End the MLflow run\n",
    "        mlflow.end_run()\n",
    "\n",
    "print(\"\\nProcessing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
