{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfrg17iR6wrE"
      },
      "source": [
        "This notebook is a continuouation of LSTM Model creation (v4), you shall use the part 1 of LSTM Model v3 and continue as done in V3 model creation notebook. upload the created folder in part 1 manually to your google drive, and follow below steps with necessary adjustments in order to have LSTM Model V4\n",
        "\n",
        "below LSTM model had very low scores, further enhancements can be done to raise the overall performance\n",
        "\n",
        "this notebook can only run in Coolab Pro utilizing the available TPU\n",
        "### main differences are: class weights were created, and validation data frame was deleted, overall score is low"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aa8mJZ9wgfY4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18waMlRJBJvs",
        "outputId": "6382588d-6961-4ad6-8ef4-893241b93c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "K1MwbpEFIHH8",
        "outputId": "2f1b8afb-5b9f-4c7b-c395-35948907ce48"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c3ee7c6474cf>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Files extracted to {output_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1660\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1662\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1713\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1715\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mfdst_write\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_left\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_crc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_update_crc\u001b[0;34m(self, newdata)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0;31m# No need to compute the CRC if we don't have a reference value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m         \u001b[0;31m# Check the CRC if we're at the end of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expected_crc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#extract the LSTM_Seq zip file\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "\n",
        "# Navigate to your uploaded file location\n",
        "drive_path = '/content/drive/My Drive/Colab Notebooks/LSTM_Seq.zip'  # Adjust this path to match your upload\n",
        "output_dir = '/content/LSTM_Seq/'\n",
        "\n",
        "# Extract the zip file\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "with zipfile.ZipFile(drive_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_dir)\n",
        "\n",
        "print(f\"Files extracted to {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3RGrtLUJ5Fa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "nested_dir = \"/content/LSTM_Seq/LSTM_Seq\"\n",
        "if os.path.exists(nested_dir):\n",
        "    print(f\"Files in {nested_dir}:\")\n",
        "    print(os.listdir(nested_dir))\n",
        "else:\n",
        "    print(\"No nested directory found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "rNwx1yBxKWH4",
        "outputId": "0fd9f18e-1ccb-4022-84d4-5e18c7dc8fb7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "Destination path '/content/LSTM_Seq/train_X_14966000.npy' already exists",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4fbb3d4d0207>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Remove the now-empty nested directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Destination path '%s' already exists\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: Destination path '/content/LSTM_Seq/train_X_14966000.npy' already exists"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "nested_dir = \"/content/LSTM_Seq/LSTM_Seq\"\n",
        "output_dir = \"/content/LSTM_Seq\"\n",
        "\n",
        "# Move all files from nested_dir to output_dir\n",
        "if os.path.exists(nested_dir):\n",
        "    for file_name in os.listdir(nested_dir):\n",
        "        shutil.move(os.path.join(nested_dir, file_name), output_dir)\n",
        "\n",
        "    # Remove the now-empty nested directory\n",
        "    shutil.rmtree(nested_dir)\n",
        "\n",
        "print(f\"Files after moving:\")\n",
        "print(os.listdir(output_dir))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnuyh60FIHF1",
        "outputId": "f3ff52e4-2710-4474-a906-0d1f1ece50f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (18096012, 60, 5), y_train shape: (18096012,)\n",
            "X_test shape: (3973708, 60, 5), y_test shape: (3973708,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "\n",
        "\n",
        "def load_batches_from_dir(output_dir, output_prefix):\n",
        "    \"\"\"\n",
        "    Load all .npy files with the given prefix from the specified directory.\n",
        "    \"\"\"\n",
        "    X_files = sorted(glob.glob(os.path.join(output_dir, f\"{output_prefix}_X_*.npy\")))\n",
        "    y_files = sorted(glob.glob(os.path.join(output_dir, f\"{output_prefix}_y_*.npy\")))\n",
        "\n",
        "    X = np.concatenate([np.load(f) for f in X_files], axis=0)\n",
        "    y = np.concatenate([np.load(f) for f in y_files], axis=0)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Load train and test datasets\n",
        "output_dir = \"/content/LSTM_Seq/\"\n",
        "X_train, y_train = load_batches_from_dir(output_dir, \"train\")\n",
        "X_test, y_test = load_batches_from_dir(output_dir, \"test\")\n",
        "\n",
        "# Verify shapes\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8jG1iBFIHDC",
        "outputId": "3d9a69e1-7de1-417d-aa7d-efd0345848b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_train_encoded shape: (18096012, 4), y_test_encoded shape: (3973708, 4)\n",
            "Class mapping: {0: 0, 101: 1, 105: 2, 107: 3}\n"
          ]
        }
      ],
      "source": [
        "#step 4\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Map class labels to zero-indexed values\n",
        "unique_classes = sorted(np.unique(y_train))  # Find unique classes in sorted order\n",
        "class_mapping = {cls: idx for idx, cls in enumerate(unique_classes)}  # Map original class to new index\n",
        "\n",
        "# Apply the mapping\n",
        "y_train_mapped = np.array([class_mapping[label] for label in y_train])\n",
        "y_test_mapped = np.array([class_mapping[label] for label in y_test])\n",
        "\n",
        "# One-hot encode the remapped labels\n",
        "num_classes = len(unique_classes)  # Total number of unique classes\n",
        "y_train_encoded = to_categorical(y_train_mapped, num_classes=num_classes)\n",
        "y_test_encoded = to_categorical(y_test_mapped, num_classes=num_classes)\n",
        "\n",
        "# Verify the shapes\n",
        "print(f\"y_train_encoded shape: {y_train_encoded.shape}, y_test_encoded shape: {y_test_encoded.shape}\")\n",
        "print(f\"Class mapping: {class_mapping}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7GU9oY6QbwV",
        "outputId": "3d11b259-d02b-4b58-83a2-44d1a8d3b7b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU: \n",
            "Number of replicas in sync: 8\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # Detect TPU\n",
        "    print(f\"Running on TPU: {tpu.master()}\")\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)  # Create a strategy\n",
        "except ValueError as e:\n",
        "    print(\"TPU not found. Falling back to CPU/GPU.\")\n",
        "    print(\"Error:\", e)\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"Number of replicas in sync:\", strategy.num_replicas_in_sync)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEKP-6HLQ1GA",
        "outputId": "809f3311-8db4-48f7-f468-5e952f1f6c3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current precision policy: <Policy \"float32\">\n",
            "X_train shape: (18096012, 60, 5), y_train_encoded shape: (18096012, 4)\n",
            "X_test shape: (3973708, 60, 5), y_test_encoded shape: (3973708, 4)\n",
            "Computed Class Weights: {0: 0.7851000125470144, 1: 1.1048112004923294, 2: 2.281798382768903, 3: 0.7231211769029111}\n",
            "Running on TPU: \n",
            "Number of replicas in sync: 8\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Reset precision policy to float32 for TPU compatibility\n",
        "set_global_policy('float32')\n",
        "\n",
        "# Verify the current precision policy\n",
        "print(\"Current precision policy:\", tf.keras.mixed_precision.global_policy())\n",
        "\n",
        "# Validate the datasets\n",
        "print(f\"X_train shape: {X_train.shape}, y_train_encoded shape: {y_train_encoded.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test_encoded shape: {y_test_encoded.shape}\")\n",
        "\n",
        "# Compute class weights\n",
        "unique_classes = np.unique(y_train_mapped)\n",
        "class_weights = compute_class_weight('balanced', classes=unique_classes, y=y_train_mapped)\n",
        "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "print(\"Computed Class Weights:\", class_weights_dict)\n",
        "\n",
        "# Use TensorFlow Dataset for efficient data pipeline\n",
        "batch_size = 1500  # Adjust based on TPU capabilities\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train_encoded))\n",
        "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test_encoded))\n",
        "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# TPU setup\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # Detect TPU\n",
        "    print('Running on TPU:', tpu.master())\n",
        "\n",
        "    # Initialize only if TPU is not already initialized\n",
        "    if not tf.config.list_logical_devices('TPU'):\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)  # Create a strategy\n",
        "except ValueError as e:\n",
        "    print(\"TPU not found. Falling back to CPU/GPU.\")\n",
        "    print(\"Error:\", e)\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"Number of replicas in sync:\", strategy.num_replicas_in_sync)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM model within the TPU strategy scope\n",
        "with strategy.scope():\n",
        "    model = Sequential([\n",
        "        LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),\n",
        "        Dropout(0.2),\n",
        "        LSTM(32, return_sequences=False),\n",
        "        Dropout(0.2),\n",
        "        Dense(num_classes, activation='softmax')  # Softmax for multi-class classification\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks for efficient training\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_model_tpu.h5', monitor='loss', save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, min_lr=1e-5)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=50,\n",
        "    class_weight=class_weights_dict,  # Add class weights\n",
        "    callbacks=[early_stopping, checkpoint, reduce_lr]\n",
        ")\n",
        "\n",
        "# Save the final model\n",
        "model.save(\"final_model_tpu.h5\")\n"
      ],
      "metadata": {
        "id": "pxgWw34NNAQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePC2jw54IG9l",
        "outputId": "6e58487d-e44d-42d9-e95c-002e18046635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model - Test Loss: 1.1992, Test Accuracy: 0.7719\n",
            "Final Model - Test Loss: 1.1992, Test Accuracy: 0.7719\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the best saved model\n",
        "best_model = load_model('best_model_tpu.h5')\n",
        "\n",
        "# Evaluate the best model\n",
        "best_test_loss, best_test_accuracy = best_model.evaluate(X_test, y_test_encoded, verbose=0)\n",
        "print(f\"Best Model - Test Loss: {best_test_loss:.4f}, Test Accuracy: {best_test_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# Load the final model\n",
        "final_model = load_model('final_model_tpu.h5')  # Replace 'final_model_tpu.h5' with your actual file name\n",
        "\n",
        "# Evaluate the final model\n",
        "final_test_loss, final_test_accuracy = final_model.evaluate(X_test, y_test_encoded, verbose=0)\n",
        "print(f\"Final Model - Test Loss: {final_test_loss:.4f}, Test Accuracy: {final_test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Gyo1Lnowq-LM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the batch size for evaluation\n",
        "batch_size = 2048  # Adjust this value based on memory availability\n",
        "\n",
        "# Create the test dataset\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test_encoded))\n",
        "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Xmg_bJu6IG6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb8552fc-acde-4810-8644-566de8ac3f2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Labels Shape: (3973708,)\n",
            "Best Model Predictions Shape: (3973708,)\n",
            "Final Model Predictions Shape: (3973708,)\n",
            "\n",
            "Classification Report for Best Model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.72      0.71   1369753\n",
            "           1       0.85      0.91      0.88   1164386\n",
            "           2       0.58      0.11      0.18    440719\n",
            "           3       0.78      0.98      0.87    998850\n",
            "\n",
            "    accuracy                           0.77   3973708\n",
            "   macro avg       0.73      0.68      0.66   3973708\n",
            "weighted avg       0.75      0.77      0.74   3973708\n",
            "\n",
            "\n",
            "Classification Report for Final Model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.72      0.71   1369753\n",
            "           1       0.85      0.91      0.88   1164386\n",
            "           2       0.58      0.11      0.18    440719\n",
            "           3       0.78      0.98      0.87    998850\n",
            "\n",
            "    accuracy                           0.77   3973708\n",
            "   macro avg       0.73      0.68      0.66   3973708\n",
            "weighted avg       0.75      0.77      0.74   3973708\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the best saved model\n",
        "best_model = load_model('best_model_tpu.h5')\n",
        "\n",
        "# Predict with the best saved model\n",
        "y_pred_probs_best = best_model.predict(test_dataset, verbose=0)\n",
        "y_pred_best = np.argmax(y_pred_probs_best, axis=1)\n",
        "\n",
        "# Load the final model (if it is saved after training)\n",
        "try:\n",
        "    final_model = load_model('final_model_tpu.h5')  # Ensure this file exists\n",
        "    y_pred_probs_final = final_model.predict(test_dataset, verbose=0)\n",
        "    y_pred_final = np.argmax(y_pred_probs_final, axis=1)\n",
        "except OSError:\n",
        "    print(\"Final model file not found. Ensure 'final_model_tpu.h5' exists.\")\n",
        "    y_pred_probs_final = None\n",
        "    y_pred_final = None\n",
        "\n",
        "# Get true labels from the test dataset\n",
        "y_true = np.argmax(y_test_encoded, axis=1)\n",
        "\n",
        "# Print shapes for verification\n",
        "print(f\"True Labels Shape: {y_true.shape}\")\n",
        "print(f\"Best Model Predictions Shape: {y_pred_best.shape}\")\n",
        "if y_pred_final is not None:\n",
        "    print(f\"Final Model Predictions Shape: {y_pred_final.shape}\")\n",
        "\n",
        "# Classification Report for the Best Model\n",
        "print(\"\\nClassification Report for Best Model:\")\n",
        "print(classification_report(y_true, y_pred_best, target_names=[str(cls) for cls in unique_classes]))\n",
        "\n",
        "# Classification Report for the Final Model\n",
        "if y_pred_final is not None:\n",
        "    print(\"\\nClassification Report for Final Model:\")\n",
        "    print(classification_report(y_true, y_pred_final, target_names=[str(cls) for cls in unique_classes]))\n",
        "else:\n",
        "    print(\"\\nFinal model predictions are unavailable.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Staking ..\n",
        "\n",
        "# Step 2: Train additional models using LSTM predictions as meta-features\n",
        "print(\"Training additional models for stacking...\")\n",
        "\n",
        "# Use LSTM predictions as input features for stacking models\n",
        "X_train_meta = best_model.predict(X_train, verbose=1)  # Meta-features for training\n",
        "X_test_meta = y_pred_probs_best  # Meta-features for testing (from the best model predictions)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "_3HAcsquvM4b",
        "outputId": "0f104a95-e43c-48ea-f6e9-ddf3a8a38145"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training additional models for stacking...\n",
            "565501/565501 [==============================] - 7381s 13ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'RandomForestClassifier' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6d34f92345f9>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Random Forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_mapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0my_test_probs_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "# Random Forest\n",
        "print(\"Training Random Forest...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "rf_model.fit(X_train_meta, y_train_mapped)\n",
        "print(\"Predicting probabilities with Random Forest...\")\n",
        "y_test_probs_rf = rf_model.predict_proba(X_test_meta)\n",
        "\n",
        "# Logistic Regression\n",
        "print(\"Training Logistic Regression...\")\n",
        "lr_model = LogisticRegression(class_weight='balanced', max_iter=1000, verbose=1)\n",
        "lr_model.fit(X_train_meta, y_train_mapped)\n",
        "print(\"Predicting probabilities with Logistic Regression...\")\n",
        "y_test_probs_lr = lr_model.predict_proba(X_test_meta)\n",
        "\n",
        "# Step 3: Combine predictions from all models\n",
        "print(\"Combining predictions using stacking...\")\n",
        "y_test_probs_combined = (y_pred_probs_best + y_test_probs_rf + y_test_probs_lr) / 3\n",
        "y_test_stacked = np.argmax(y_test_probs_combined, axis=1)\n",
        "\n",
        "# Step 4: Evaluate the stacked model\n",
        "y_true = y_test_mapped  # True labels\n",
        "\n",
        "print(\"\\nClassification Report for Stacked Model:\")\n",
        "print(classification_report(y_true, y_test_stacked, target_names=[str(cls) for cls in unique_classes]))\n",
        "\n",
        "# Optional: Save the stacked model predictions for further analysis\n",
        "np.save(\"stacked_predictions.npy\", y_test_probs_combined)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tk6ikEUINUON",
        "outputId": "d458c161-e175-4ab3-ca71-538dde6f60c1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Random Forest...\n",
            "Predicting probabilities with Random Forest...\n",
            "Training Logistic Regression...\n",
            "Predicting probabilities with Logistic Regression...\n",
            "Combining predictions using stacking...\n",
            "\n",
            "Classification Report for Stacked Model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.73      0.81   1369753\n",
            "           1       0.81      0.92      0.86   1164386\n",
            "           2       0.91      0.78      0.84    440719\n",
            "           3       0.78      0.94      0.85    998850\n",
            "\n",
            "    accuracy                           0.84   3973708\n",
            "   macro avg       0.86      0.84      0.84   3973708\n",
            "weighted avg       0.85      0.84      0.84   3973708\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zRfD5fKwvM1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qg64oKv9vMyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rmqMddG5vMv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pPmnEM_8vMt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tyC1inh5vMrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H3-GIgtavMoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8022vZS2JUE8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}